{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Navier–Stokes PDE solver (pseudo-spectral method)\n",
    "# =============================================================================\n",
    "def solve_navier_stokes(w0, nu, dt, t_final, N):\n",
    "    \"\"\"\n",
    "    Solves the 2D Navier–Stokes vorticity PDE with periodic boundary conditions\n",
    "    using a pseudo-spectral method.\n",
    "    \n",
    "    PDE:\n",
    "        ∂ₜw + u · ∇w = ν ∆w + f(x)\n",
    "        u = (ψ_y, -ψ_x),   with  ∆ψ = -w.\n",
    "    \n",
    "    Inputs:\n",
    "        w0: initial vorticity, shape (N, N)\n",
    "        nu: viscosity\n",
    "        dt: time step\n",
    "        t_final: final time\n",
    "        N: grid size (N×N)\n",
    "    Outputs:\n",
    "        snapshots: list of (t, w) tuples recorded at integer times.\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 1, N, endpoint=False)\n",
    "    y = np.linspace(0, 1, N, endpoint=False)\n",
    "    xv, yv = np.meshgrid(x, y, indexing='ij')\n",
    "    f = 0.1 * (np.sin(2*np.pi*(xv + yv)) + np.cos(2*np.pi*(xv + yv)))\n",
    "    kx = np.fft.fftfreq(N, d=1.0/N) * 2*np.pi\n",
    "    ky = np.fft.fftfreq(N, d=1.0/N) * 2*np.pi\n",
    "    kx, ky = np.meshgrid(kx, ky, indexing='ij')\n",
    "    ksq = kx**2 + ky**2\n",
    "    ksq[0,0] = 1.0\n",
    "    w = w0.copy()\n",
    "    snapshots = []\n",
    "    snapshot_times = np.arange(0, t_final+dt, 1.0)\n",
    "    next_snapshot_index = 0\n",
    "    nsteps = int(t_final/dt)\n",
    "    for step in range(nsteps):\n",
    "        t = step * dt\n",
    "        if next_snapshot_index < len(snapshot_times) and t >= snapshot_times[next_snapshot_index]:\n",
    "            snapshots.append((t, w.copy()))\n",
    "            next_snapshot_index += 1\n",
    "        w_hat = np.fft.fft2(w)\n",
    "        psi_hat = -w_hat / ksq\n",
    "        psi_hat[0,0] = 0.0\n",
    "        psi = np.real(np.fft.ifft2(psi_hat))\n",
    "        u = np.real(np.fft.ifft2(1j * ky * psi_hat))\n",
    "        v = -np.real(np.fft.ifft2(1j * kx * psi_hat))\n",
    "        w_x = np.real(np.fft.ifft2(1j * kx * w_hat))\n",
    "        w_y = np.real(np.fft.ifft2(1j * ky * w_hat))\n",
    "        nonlinear = u * w_x + v * w_y\n",
    "        lap_w = np.real(np.fft.ifft2(-ksq * w_hat))\n",
    "        w = w + dt * (-nonlinear + nu * lap_w + f)\n",
    "    return snapshots\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Navier-Stokes Dataset (using the PDE solver)\n",
    "# =============================================================================\n",
    "class NavierStokesDatasetFromSolver(Dataset):\n",
    "    def __init__(self, num_samples, N, dt, t_final, nu, T_in_range, T_out_times):\n",
    "        \"\"\"\n",
    "        num_samples: number of samples.\n",
    "        N: grid size (N×N).\n",
    "        dt: time step.\n",
    "        t_final: final time (e.g., 50).\n",
    "        nu: viscosity.\n",
    "        T_in_range: input time range (e.g., (0,10)).\n",
    "        T_out_times: target times (e.g., [20,30,40,49]).\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        for i in range(num_samples):\n",
    "            w0 = np.random.randn(N, N)\n",
    "            snapshots = solve_navier_stokes(w0, nu, dt, t_final, N)\n",
    "            input_series = []\n",
    "            for t, w in snapshots:\n",
    "                if t >= T_in_range[0] and t <= T_in_range[1]:\n",
    "                    input_series.append(w)\n",
    "            input_series = np.array(input_series)  # (T_in, N, N)\n",
    "            target_series = []\n",
    "            for t, w in snapshots:\n",
    "                if t in T_out_times:\n",
    "                    target_series.append(w)\n",
    "            target_series = np.array(target_series)  # (T_out, N, N)\n",
    "            self.samples.append((input_series, target_series))\n",
    "        x = np.linspace(0, 1, N, endpoint=False)\n",
    "        y = np.linspace(0, 1, N, endpoint=False)\n",
    "        xv, yv = np.meshgrid(x, y, indexing='ij')\n",
    "        self.spatial_coords = np.stack([xv, yv], axis=-1)  # (N, N, 2)\n",
    "        self.input_times = np.arange(T_in_range[0], T_in_range[1]+1)\n",
    "        self.target_times = np.array(T_out_times)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_series, target_series = self.samples[idx]\n",
    "        T_in = input_series.shape[0]\n",
    "        N = input_series.shape[1]\n",
    "        input_data = []\n",
    "        for i, t in enumerate(self.input_times):\n",
    "            t_field = np.full((N, N, 1), t)\n",
    "            w_field = input_series[i][:, :, np.newaxis]\n",
    "            combined = np.concatenate([w_field, t_field, self.spatial_coords], axis=-1)\n",
    "            input_data.append(combined)\n",
    "        input_data = np.stack(input_data, axis=0)  # (T_in, N, N, 4)\n",
    "        if target_series.shape[0] < len(self.target_times):\n",
    "            pad_len = len(self.target_times) - target_series.shape[0]\n",
    "            last = target_series[-1:,...]\n",
    "            pad = np.repeat(last, pad_len, axis=0)\n",
    "            target_series = np.concatenate([target_series, pad], axis=0)\n",
    "        target_data = []\n",
    "        for i in range(target_series.shape[0]):\n",
    "            target_data.append(target_series[i][:, :, np.newaxis])\n",
    "        target_data = np.stack(target_data, axis=0)  # (T_out, N, N, 1)\n",
    "        input_data = torch.tensor(input_data, dtype=torch.float32)\n",
    "        target_data = torch.tensor(target_data, dtype=torch.float32)\n",
    "        return input_data, target_data\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FNO Model (3D FNO: Space-Time Operator Learning) with Physics-Informed Loss\n",
    "# =============================================================================\n",
    "class SpectralConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes_t, modes_x, modes_y):\n",
    "        super(SpectralConv3d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes_t = modes_t\n",
    "        self.modes_x = modes_x\n",
    "        self.modes_y = modes_y\n",
    "        self.scale = 1 / (in_channels * out_channels)\n",
    "        self.weights = nn.Parameter(\n",
    "            self.scale * torch.rand(in_channels, out_channels, modes_t, modes_x, modes_y, 2)\n",
    "        )\n",
    "    \n",
    "    def compl_mul3d(self, input, weights):\n",
    "        weight_complex = torch.view_as_complex(weights)\n",
    "        return torch.einsum(\"bctxy, cotxy -> botxy\", input, weight_complex)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        x_ft = torch.fft.rfftn(x, dim=[2,3,4])\n",
    "        out_ft = torch.zeros(\n",
    "            batchsize, self.out_channels, x_ft.size(2), x_ft.size(3), x_ft.size(4),\n",
    "            device=x.device, dtype=torch.cfloat\n",
    "        )\n",
    "        t_modes = min(self.modes_t, x_ft.size(2))\n",
    "        x_modes = min(self.modes_x, x_ft.size(3))\n",
    "        y_modes = min(self.modes_y, x_ft.size(4))\n",
    "        out_ft[:, :, :t_modes, :x_modes, :y_modes] = self.compl_mul3d(\n",
    "            x_ft[:, :, :t_modes, :x_modes, :y_modes],\n",
    "            self.weights[:, :, :t_modes, :x_modes, :y_modes]\n",
    "        )\n",
    "        x = torch.fft.irfftn(out_ft, s=(x.size(2), x.size(3), x.size(4)))\n",
    "        return x\n",
    "\n",
    "class FNO3d(nn.Module):\n",
    "    def __init__(self, modes_t, modes_x, modes_y, width, T_in, T_out):\n",
    "        \"\"\"\n",
    "        modes_t, modes_x, modes_y: Fourier modes along time and space.\n",
    "        width: latent channel dimension.\n",
    "        T_in: number of input time steps (e.g., 11 for t=0~10)\n",
    "        T_out: number of output time steps (e.g., 4 for t=15,20,25,30)\n",
    "        \"\"\"\n",
    "        super(FNO3d, self).__init__()\n",
    "        self.width = width\n",
    "        self.T_in = T_in\n",
    "        self.T_out = T_out\n",
    "        self.fc0 = nn.Linear(4, width)  # input: [vorticity, t, x, y]\n",
    "        \n",
    "        self.conv0 = SpectralConv3d(width, width, modes_t, modes_x, modes_y)\n",
    "        self.conv1 = SpectralConv3d(width, width, modes_t, modes_x, modes_y)\n",
    "        self.conv2 = SpectralConv3d(width, width, modes_t, modes_x, modes_y)\n",
    "        self.conv3 = SpectralConv3d(width, width, modes_t, modes_x, modes_y)\n",
    "        \n",
    "        self.w0 = nn.Conv3d(width, width, 1)\n",
    "        self.w1 = nn.Conv3d(width, width, 1)\n",
    "        self.w2 = nn.Conv3d(width, width, 1)\n",
    "        self.w3 = nn.Conv3d(width, width, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)  # output: vorticity\n",
    "        \n",
    "        # Temporal predictor: map input time dimension T_in to output time dimension T_out per pixel\n",
    "        self.temporal_fc = nn.Linear(T_in, T_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, T_in, H, W, 4)\n",
    "        batch, T_in, H, W, _ = x.shape\n",
    "        x = self.fc0(x)  # (batch, T_in, H, W, width)\n",
    "        x = x.permute(0, 4, 1, 2, 3)  # (batch, width, T_in, H, W)\n",
    "        \n",
    "        x0 = self.conv0(x)\n",
    "        x1 = self.w0(x)\n",
    "        x = F.gelu(x0 + x1)\n",
    "        \n",
    "        x0 = self.conv1(x)\n",
    "        x1 = self.w1(x)\n",
    "        x = F.gelu(x0 + x1)\n",
    "        \n",
    "        x0 = self.conv2(x)\n",
    "        x1 = self.w2(x)\n",
    "        x = F.gelu(x0 + x1)\n",
    "        \n",
    "        x0 = self.conv3(x)\n",
    "        x1 = self.w3(x)\n",
    "        x = F.gelu(x0 + x1)\n",
    "        \n",
    "        x = x.permute(0, 2, 3, 4, 1)  # (batch, T_in, H, W, width)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)  # (batch, T_in, H, W, 1)\n",
    "        \n",
    "        # Temporal prediction: map the time dimension from T_in to T_out per pixel\n",
    "        x = x.squeeze(-1)         # (batch, T_in, H, W)\n",
    "        x = x.permute(0, 2, 3, 1)   # (batch, H, W, T_in)\n",
    "        x = self.temporal_fc(x)     # (batch, H, W, T_out)\n",
    "        x = x.permute(0, 3, 1, 2).unsqueeze(-1)  # (batch, T_out, H, W, 1)\n",
    "        return x\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Physics-Informed Loss Functions using autograd\n",
    "# =============================================================================\n",
    "def compute_PDE_loss_autograd(pred, nu):\n",
    "    \"\"\"\n",
    "    Computes the PDE loss for the predicted vorticity using autograd to obtain\n",
    "    time and spatial derivatives.\n",
    "    \n",
    "    pred: (batch, T_out, H, W, 1) predicted vorticity from the network.\n",
    "    nu: viscosity coefficient.\n",
    "    \n",
    "    PDE: ∂ₜw + u · ∇w = ν Δw + f(x), with\n",
    "         f(x) = 0.1*(sin(2π(x+y)) + cos(2π(x+y))).\n",
    "    \n",
    "    This function creates coordinate tensors on the output grid with requires_grad=True,\n",
    "    then uses torch.autograd.grad to compute derivatives of the predicted vorticity with respect\n",
    "    to time (t) and space (x, y). The PDE residual is computed and its MSE is returned.\n",
    "    \"\"\"\n",
    "    batch, T_out, H, W, _ = pred.shape\n",
    "    w = pred.squeeze(-1)  # (batch, T_out, H, W)\n",
    "    device = pred.device\n",
    "\n",
    "    # Create coordinate tensors with requires_grad=True\n",
    "    t_coords = torch.linspace(0, 1, steps=T_out, device=device, requires_grad=True).view(1, T_out, 1, 1).expand(batch, T_out, H, W)\n",
    "    x_coords = torch.linspace(0, 1, steps=W, device=device, requires_grad=True).view(1, 1, 1, W).expand(batch, T_out, H, W)\n",
    "    y_coords = torch.linspace(0, 1, steps=H, device=device, requires_grad=True).view(1, 1, H, 1).expand(batch, T_out, H, W)\n",
    "\n",
    "    grad_outputs = torch.ones_like(w)\n",
    "    dw_dt = torch.autograd.grad(w, t_coords, grad_outputs=grad_outputs, create_graph=True, retain_graph=True)[0]\n",
    "    dw_dx = torch.autograd.grad(w, x_coords, grad_outputs=grad_outputs, create_graph=True, retain_graph=True)[0]\n",
    "    dw_dy = torch.autograd.grad(w, y_coords, grad_outputs=grad_outputs, create_graph=True, retain_graph=True)[0]\n",
    "    d2w_dx2 = torch.autograd.grad(dw_dx, x_coords, grad_outputs=grad_outputs, create_graph=True, retain_graph=True)[0]\n",
    "    d2w_dy2 = torch.autograd.grad(dw_dy, y_coords, grad_outputs=grad_outputs, create_graph=True, retain_graph=True)[0]\n",
    "    lap = d2w_dx2 + d2w_dy2\n",
    "\n",
    "    # Compute velocity using FFT-based method (autograd compatible)\n",
    "    u, v = compute_velocity_torch(w)  # each: (batch, H, W) - computed per frame (assume same for all T_out)\n",
    "    # For proper broadcasting, expand u and v to (batch, T_out, H, W)\n",
    "    u = u.unsqueeze(1).expand(-1, T_out, -1, -1)\n",
    "    v = v.unsqueeze(1).expand(-1, T_out, -1, -1)\n",
    "    \n",
    "    adv = u * dw_dx + v * dw_dy\n",
    "    f_val = 0.1 * (torch.sin(2*np.pi*(x_coords + y_coords)) + torch.cos(2*np.pi*(x_coords + y_coords)))\n",
    "    r = dw_dt + adv - nu * lap - f_val\n",
    "    loss_pde = torch.mean(r**2)\n",
    "    return loss_pde\n",
    "\n",
    "def compute_BC_loss(pred):\n",
    "    \"\"\"\n",
    "    Computes the boundary condition loss for predicted vorticity, enforcing periodic boundaries.\n",
    "    pred: (batch, T_out, H, W, 1)\n",
    "    \"\"\"\n",
    "    batch, T_out, H, W, _ = pred.shape\n",
    "    pred = pred.squeeze(-1)\n",
    "    loss_bc = 0.0\n",
    "    bc_left = pred[:, :, :, 0]\n",
    "    bc_right = pred[:, :, :, -1]\n",
    "    loss_bc += torch.mean((bc_left - bc_right)**2)\n",
    "    bc_top = pred[:, :, 0, :]\n",
    "    bc_bottom = pred[:, :, -1, :]\n",
    "    loss_bc += torch.mean((bc_top - bc_bottom)**2)\n",
    "    return loss_bc\n",
    "\n",
    "def compute_IC_loss(pred, ic):\n",
    "    \"\"\"\n",
    "    Computes the initial condition loss.\n",
    "    pred: (batch, T_out, H, W, 1) - we use the first predicted frame.\n",
    "    ic: (batch, T_out, H, W, 1) - we assume the first frame is the initial condition.\n",
    "    \"\"\"\n",
    "    return F.mse_loss(pred[:,0,:,:,:], ic[:,0,:,:,:])\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Training Loop with Physics-Informed Loss (using autograd for PDE loss)\n",
    "# =============================================================================\n",
    "def train_model_3d(model, train_loader, test_loader, epochs=300, lambda_phys=1.0, dt_out=1.0):\n",
    "    \"\"\"\n",
    "    lambda_phys: weight for the physics loss.\n",
    "    dt_out: time interval for the output (e.g., 1.0 if target times are integers)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Assume spatial grid for 64x64\n",
    "    H_grid = 64\n",
    "    W_grid = 64\n",
    "    dx = 1.0 / W_grid\n",
    "    dy = 1.0 / H_grid\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for input_data, target in train_loader:\n",
    "            # input_data: (batch, T_in, H, W, 4), target: (batch, T_out, H, W, 1)\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_data)  # (batch, T_out, H, W, 1)\n",
    "            loss_data = loss_fn(output, target)\n",
    "            loss_phys = compute_PDE_loss_autograd(output, nu)\n",
    "            loss_bc = compute_BC_loss(output)\n",
    "            loss_ic = compute_IC_loss(output, target)\n",
    "            loss = loss_data + lambda_phys * loss_phys + loss_bc + loss_ic\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * input_data.size(0)\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(total_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss_test = 0.0\n",
    "        with torch.no_grad():\n",
    "            for input_data, target in test_loader:\n",
    "                input_data = input_data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(input_data)\n",
    "                loss_data = loss_fn(output, target)\n",
    "                loss_phys = compute_PDE_loss_autograd(output, nu)\n",
    "                loss_bc = compute_BC_loss(output)\n",
    "                loss_ic = compute_IC_loss(output, target)\n",
    "                loss = loss_data + lambda_phys * loss_phys + loss_bc + loss_ic\n",
    "                total_loss_test += loss.item() * input_data.size(0)\n",
    "        total_loss_test /= len(test_loader.dataset)\n",
    "        test_losses.append(total_loss_test)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch:03d}: Train Loss = {total_loss:.6f}, Test Loss = {total_loss_test:.6f}\")\n",
    "    \n",
    "        if epoch % 100 == 0:\n",
    "            plt.figure(figsize=(8,6))\n",
    "            plt.plot(train_losses, label=\"Train Loss\")\n",
    "            plt.plot(test_losses, label=\"Test Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Total Loss\")\n",
    "            plt.yscale('log')\n",
    "            plt.legend()\n",
    "            plt.title(\"Training and Test Loss Curves (FNO3d with Physics Loss) [Log Scale]\")\n",
    "            plt.show()\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Fine-tuning using L-BFGS optimizer (with physics-informed loss)\n",
    "# =============================================================================\n",
    "def fine_tune_lbfgs(model, train_loader, test_loader, epochs=50, lambda_phys=1.0, dt_out=1.0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), lr=0.1, max_iter=20)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    H_grid = 64\n",
    "    W_grid = 64\n",
    "    dx = 1.0 / W_grid\n",
    "    dy = 1.0 / H_grid\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        total_loss_train = 0.0\n",
    "        for input_data, target in train_loader:\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                output = model(input_data)\n",
    "                loss_data = loss_fn(output, target)\n",
    "                loss_phys = compute_PDE_loss_autograd(output, nu)\n",
    "                loss_bc = compute_BC_loss(output)\n",
    "                loss_ic = compute_IC_loss(output, target)\n",
    "                loss = loss_data + lambda_phys * loss_phys + loss_bc + loss_ic\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            loss = optimizer.step(closure)\n",
    "            total_loss_train += loss.item() * input_data.size(0)\n",
    "        total_loss_train /= len(train_loader.dataset)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        total_loss_test = 0.0\n",
    "        with torch.no_grad():\n",
    "            for input_data, target in test_loader:\n",
    "                input_data = input_data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(input_data)\n",
    "                loss_data = loss_fn(output, target)\n",
    "                loss_phys = compute_PDE_loss_autograd(output, nu)\n",
    "                loss_bc = compute_BC_loss(output)\n",
    "                loss_ic = compute_IC_loss(output, target)\n",
    "                loss = loss_data + lambda_phys * loss_phys + loss_bc + loss_ic\n",
    "                total_loss_test += loss.item() * input_data.size(0)\n",
    "        total_loss_test /= len(test_loader.dataset)\n",
    "        model.train()\n",
    "        print(f\"L-BFGS Epoch {epoch:03d}: Train Loss = {total_loss_train:.6f}, Test Loss = {total_loss_test:.6f}\")\n",
    "    return total_loss_train, total_loss_test\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Discrete Visualization\n",
    "# =============================================================================\n",
    "def visualize_sample_3d(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_data, target = next(iter(test_loader))\n",
    "        input_data = input_data.to(device)\n",
    "        pred = model(input_data)\n",
    "    sample_target = target[0].cpu().numpy()  # (T_out, H, W, 1)\n",
    "    sample_pred = pred[0].cpu().numpy()      # (T_out, H, W, 1)\n",
    "    times = [20, 30, 40, 49]  # target time labels\n",
    "    T_out = sample_target.shape[0]\n",
    "    plt.figure(figsize=(16, 4*T_out))\n",
    "    for i in range(T_out):\n",
    "        plt.subplot(T_out, 2, 2*i+1)\n",
    "        plt.title(f\"Ground Truth at t = {times[i]}\")\n",
    "        plt.imshow(sample_target[i, :, :, 0], cmap=\"RdBu\", origin=\"lower\")\n",
    "        plt.colorbar()\n",
    "        plt.subplot(T_out, 2, 2*i+2)\n",
    "        plt.title(f\"Prediction at t = {times[i]}\")\n",
    "        plt.imshow(sample_pred[i, :, :, 0], cmap=\"RdBu\", origin=\"lower\")\n",
    "        plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 9. Full Pipeline Execution\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # PDE simulation parameters\n",
    "    N = 64         # Spatial resolution: 64x64\n",
    "    dt = 0.005     # Time step (small for stability)\n",
    "    t_final = 50   # Final time\n",
    "    nu = 1e-3      # Viscosity\n",
    "    \n",
    "    # Dataset time ranges\n",
    "    T_in_range = (0, 10)          # Input: t=0 ~ 10 (integer snapshots)\n",
    "    T_out_times = [20, 30, 40, 49] # Target times (example)\n",
    "    \n",
    "    num_train = 10   # Number of training samples\n",
    "    num_test = 2      # Number of test samples\n",
    "    \n",
    "    dataset_solver_train = NavierStokesDatasetFromSolver(num_samples=num_train, N=N, dt=dt, t_final=t_final, nu=nu, \n",
    "                                                          T_in_range=T_in_range, T_out_times=T_out_times)\n",
    "    dataset_solver_test  = NavierStokesDatasetFromSolver(num_samples=num_test, N=N, dt=dt, t_final=t_final, nu=nu, \n",
    "                                                          T_in_range=T_in_range, T_out_times=T_out_times)\n",
    "    \n",
    "    train_loader = DataLoader(dataset_solver_train, batch_size=2, shuffle=True)\n",
    "    test_loader = DataLoader(dataset_solver_test, batch_size=2, shuffle=False)\n",
    "    \n",
    "    modes_t, modes_x, modes_y = 4, 16, 16\n",
    "    width = 32\n",
    "    T_out = len(T_out_times)\n",
    "    T_in = len(np.arange(T_in_range[0], T_in_range[1]+1))  # e.g., 11 for t=0~10\n",
    "    model = FNO3d(modes_t, modes_x, modes_y, width, T_in, T_out)\n",
    "    \n",
    "    # Pre-training with Adam optimizer using autograd-based PDE loss\n",
    "    train_losses, test_losses = train_model_3d(model, train_loader, test_loader, epochs=500, lambda_phys=1.0, dt_out=1.0)\n",
    "    \n",
    "    # Fine-tuning with L-BFGS optimizer using autograd-based PDE loss\n",
    "    print(\"Starting L-BFGS fine-tuning...\")\n",
    "    fine_tune_lbfgs(model, train_loader, test_loader, epochs=50, lambda_phys=1.0, dt_out=1.0)\n",
    "    \n",
    "    # Visualize discrete sample frames\n",
    "    visualize_sample_3d(model, test_loader)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 546\u001b[0m\n\u001b[0;32m    543\u001b[0m model \u001b[38;5;241m=\u001b[39m FNO3d(modes_t, modes_x, modes_y, width, T_in, T_out)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# Pre-training with Adam optimizer using autograd-based PDE loss\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_phys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# Fine-tuning with L-BFGS optimizer using autograd-based PDE loss, BC loss and IC loss\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting L-BFGS fine-tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 391\u001b[0m, in \u001b[0;36mtrain_model_3d\u001b[1;34m(model, train_loader, test_loader, epochs, lambda_phys, lambda_bc, lambda_ic, dt_out)\u001b[0m\n\u001b[0;32m    389\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_data)  \u001b[38;5;66;03m# (batch, T_out, H, W, 1)\u001b[39;00m\n\u001b[0;32m    390\u001b[0m loss_data \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[1;32m--> 391\u001b[0m loss_phys \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_PDE_loss_autograd\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m loss_bc \u001b[38;5;241m=\u001b[39m compute_BC_loss(output)\n\u001b[0;32m    393\u001b[0m loss_ic \u001b[38;5;241m=\u001b[39m compute_IC_loss(output, target)\n",
      "Cell \u001b[1;32mIn[3], line 307\u001b[0m, in \u001b[0;36mcompute_PDE_loss_autograd\u001b[1;34m(pred, nu)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dw_dy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     dw_dy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(w)\n\u001b[1;32m--> 307\u001b[0m d2w_dx2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdw_dx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m d2w_dx2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     d2w_dx2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(w)\n",
      "File \u001b[1;32mc:\\Users\\sbeen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:412\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    409\u001b[0m         grad_outputs_\n\u001b[0;32m    410\u001b[0m     )\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    423\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    425\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\sbeen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Navier–Stokes PDE solver (pseudo-spectral method)\n",
    "# =============================================================================\n",
    "def solve_navier_stokes(w0, nu, dt, t_final, N):\n",
    "    \"\"\"\n",
    "    Solves the 2D Navier–Stokes vorticity PDE with periodic boundary conditions\n",
    "    using a pseudo-spectral method.\n",
    "    \n",
    "    PDE:\n",
    "        ∂ₜw + u · ∇w = ν ∆w + f(x)\n",
    "        u = (ψ_y, -ψ_x),   with  ∆ψ = -w.\n",
    "    \n",
    "    Inputs:\n",
    "        w0: initial vorticity, shape (N, N)\n",
    "        nu: viscosity\n",
    "        dt: time step\n",
    "        t_final: final time\n",
    "        N: grid size (N×N)\n",
    "    Outputs:\n",
    "        snapshots: list of (t, w) tuples recorded at integer times.\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 1, N, endpoint=False)\n",
    "    y = np.linspace(0, 1, N, endpoint=False)\n",
    "    xv, yv = np.meshgrid(x, y, indexing='ij')\n",
    "    f = 0.1 * (np.sin(2*np.pi*(xv + yv)) + np.cos(2*np.pi*(xv + yv)))\n",
    "    kx = np.fft.fftfreq(N, d=1.0/N) * 2*np.pi\n",
    "    ky = np.fft.fftfreq(N, d=1.0/N) * 2*np.pi\n",
    "    kx, ky = np.meshgrid(kx, ky, indexing='ij')\n",
    "    ksq = kx**2 + ky**2\n",
    "    ksq[0,0] = 1.0  # avoid division by zero\n",
    "    \n",
    "    w = w0.copy()\n",
    "    snapshots = []\n",
    "    snapshot_times = np.arange(0, t_final+dt, 1.0)\n",
    "    next_snapshot_index = 0\n",
    "    nsteps = int(t_final/dt)\n",
    "    for step in range(nsteps):\n",
    "        t = step * dt\n",
    "        if next_snapshot_index < len(snapshot_times) and t >= snapshot_times[next_snapshot_index]:\n",
    "            snapshots.append((t, w.copy()))\n",
    "            next_snapshot_index += 1\n",
    "        w_hat = np.fft.fft2(w)\n",
    "        psi_hat = -w_hat / ksq\n",
    "        psi_hat[0,0] = 0.0\n",
    "        psi = np.real(np.fft.ifft2(psi_hat))\n",
    "        u = np.real(np.fft.ifft2(1j * ky * psi_hat))\n",
    "        v = -np.real(np.fft.ifft2(1j * kx * psi_hat))\n",
    "        w_x = np.real(np.fft.ifft2(1j * kx * w_hat))\n",
    "        w_y = np.real(np.fft.ifft2(1j * ky * w_hat))\n",
    "        nonlinear = u * w_x + v * w_y\n",
    "        lap_w = np.real(np.fft.ifft2(-ksq * w_hat))\n",
    "        w = w + dt * (-nonlinear + nu * lap_w + f)\n",
    "    return snapshots\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Navier-Stokes Dataset (using PDE solver)\n",
    "# =============================================================================\n",
    "class NavierStokesDatasetFromSolver(Dataset):\n",
    "    def __init__(self, num_samples, N, dt, t_final, nu, T_in_range, T_out_times):\n",
    "        \"\"\"\n",
    "        num_samples: number of samples.\n",
    "        N: grid size (N×N).\n",
    "        dt: time step.\n",
    "        t_final: final time (e.g., 50).\n",
    "        nu: viscosity.\n",
    "        T_in_range: input time range (e.g., (0,10)).\n",
    "        T_out_times: target times (e.g., [20,30,40,49]).\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        for i in range(num_samples):\n",
    "            w0 = np.random.randn(N, N)\n",
    "            snapshots = solve_navier_stokes(w0, nu, dt, t_final, N)\n",
    "            input_series = []\n",
    "            for t, w in snapshots:\n",
    "                if t >= T_in_range[0] and t <= T_in_range[1]:\n",
    "                    input_series.append(w)\n",
    "            input_series = np.array(input_series)  # shape: (T_in, N, N)\n",
    "            target_series = []\n",
    "            for t, w in snapshots:\n",
    "                if t in T_out_times:\n",
    "                    target_series.append(w)\n",
    "            target_series = np.array(target_series)  # shape: (T_out, N, N)\n",
    "            self.samples.append((input_series, target_series))\n",
    "        x = np.linspace(0, 1, N, endpoint=False)\n",
    "        y = np.linspace(0, 1, N, endpoint=False)\n",
    "        xv, yv = np.meshgrid(x, y, indexing='ij')\n",
    "        self.spatial_coords = np.stack([xv, yv], axis=-1)  # shape: (N, N, 2)\n",
    "        self.input_times = np.arange(T_in_range[0], T_in_range[1]+1)\n",
    "        self.target_times = np.array(T_out_times)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_series, target_series = self.samples[idx]\n",
    "        T_in = input_series.shape[0]\n",
    "        N = input_series.shape[1]\n",
    "        input_data = []\n",
    "        for i, t in enumerate(self.input_times):\n",
    "            t_field = np.full((N, N, 1), t)\n",
    "            w_field = input_series[i][:, :, np.newaxis]\n",
    "            combined = np.concatenate([w_field, t_field, self.spatial_coords], axis=-1)\n",
    "            input_data.append(combined)\n",
    "        input_data = np.stack(input_data, axis=0)  # shape: (T_in, N, N, 4)\n",
    "        if target_series.shape[0] < len(self.target_times):\n",
    "            pad_len = len(self.target_times) - target_series.shape[0]\n",
    "            last = target_series[-1:,...]\n",
    "            pad = np.repeat(last, pad_len, axis=0)\n",
    "            target_series = np.concatenate([target_series, pad], axis=0)\n",
    "        target_data = []\n",
    "        for i in range(target_series.shape[0]):\n",
    "            target_data.append(target_series[i][:, :, np.newaxis])\n",
    "        target_data = np.stack(target_data, axis=0)  # shape: (T_out, N, N, 1)\n",
    "        input_data = torch.tensor(input_data, dtype=torch.float32)\n",
    "        target_data = torch.tensor(target_data, dtype=torch.float32)\n",
    "        return input_data, target_data\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FNO Model (3D FNO: Space-Time Operator Learning) with Physics-Informed Loss\n",
    "# =============================================================================\n",
    "class SpectralConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes_t, modes_x, modes_y):\n",
    "        super(SpectralConv3d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes_t = modes_t\n",
    "        self.modes_x = modes_x\n",
    "        self.modes_y = modes_y\n",
    "        self.scale = 1 / (in_channels * out_channels)\n",
    "        self.weights = nn.Parameter(\n",
    "            self.scale * torch.rand(in_channels, out_channels, modes_t, modes_x, modes_y, 2)\n",
    "        )\n",
    "    \n",
    "    def compl_mul3d(self, input, weights):\n",
    "        weight_complex = torch.view_as_complex(weights)\n",
    "        return torch.einsum(\"bctxy, cotxy -> botxy\", input, weight_complex)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        x_ft = torch.fft.rfftn(x, dim=[2,3,4])\n",
    "        out_ft = torch.zeros(\n",
    "            batchsize, self.out_channels, x_ft.size(2), x_ft.size(3), x_ft.size(4),\n",
    "            device=x.device, dtype=torch.cfloat\n",
    "        )\n",
    "        t_modes = min(self.modes_t, x_ft.size(2))\n",
    "        x_modes = min(self.modes_x, x_ft.size(3))\n",
    "        y_modes = min(self.modes_y, x_ft.size(4))\n",
    "        out_ft[:, :, :t_modes, :x_modes, :y_modes] = self.compl_mul3d(\n",
    "            x_ft[:, :, :t_modes, :x_modes, :y_modes],\n",
    "            self.weights[:, :, :t_modes, :x_modes, :y_modes]\n",
    "        )\n",
    "        x = torch.fft.irfftn(out_ft, s=(x.size(2), x.size(3), x.size(4)))\n",
    "        return x\n",
    "\n",
    "class FNO3d(nn.Module):\n",
    "    def __init__(self, modes_t, modes_x, modes_y, width, T_in, T_out):\n",
    "        \"\"\"\n",
    "        modes_t, modes_x, modes_y: Fourier modes along time and space.\n",
    "        width: latent channel dimension.\n",
    "        T_in: number of input time steps (e.g., 11 for t=0~10)\n",
    "        T_out: number of output time steps (e.g., 4 for t=15,20,25,30)\n",
    "        \"\"\"\n",
    "        super(FNO3d, self).__init__()\n",
    "        self.width = width\n",
    "        self.T_in = T_in\n",
    "        self.T_out = T_out\n",
    "        self.fc0 = nn.Linear(4, width)  # input: [vorticity, t, x, y]\n",
    "        \n",
    "        self.conv0 = SpectralConv3d(width, width, modes_t, modes_x, modes_y)\n",
    "        self.conv1 = SpectralConv3d(width, width, modes_t, modes_x, modes_y)\n",
    "        self.conv2 = SpectralConv3d(width, width, modes_t, modes_x, modes_y)\n",
    "        self.conv3 = SpectralConv3d(width, width, modes_t, modes_x, modes_y)\n",
    "        \n",
    "        self.w0 = nn.Conv3d(width, width, 1)\n",
    "        self.w1 = nn.Conv3d(width, width, 1)\n",
    "        self.w2 = nn.Conv3d(width, width, 1)\n",
    "        self.w3 = nn.Conv3d(width, width, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)  # output: vorticity\n",
    "        \n",
    "        # Temporal predictor: map input time dimension T_in to output time dimension T_out per pixel\n",
    "        self.temporal_fc = nn.Linear(T_in, T_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, T_in, H, W, 4)\n",
    "        batch, T_in, H, W, _ = x.shape\n",
    "        x = self.fc0(x)  # (batch, T_in, H, W, width)\n",
    "        x = x.permute(0, 4, 1, 2, 3)  # (batch, width, T_in, H, W)\n",
    "        \n",
    "        x0 = self.conv0(x)\n",
    "        x1 = self.w0(x)\n",
    "        x = F.gelu(x0 + x1)\n",
    "        \n",
    "        x0 = self.conv1(x)\n",
    "        x1 = self.w1(x)\n",
    "        x = F.gelu(x0 + x1)\n",
    "        \n",
    "        x0 = self.conv2(x)\n",
    "        x1 = self.w2(x)\n",
    "        x = F.gelu(x0 + x1)\n",
    "        \n",
    "        x0 = self.conv3(x)\n",
    "        x1 = self.w3(x)\n",
    "        x = F.gelu(x0 + x1)\n",
    "        \n",
    "        x = x.permute(0, 2, 3, 4, 1)  # (batch, T_in, H, W, width)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)  # (batch, T_in, H, W, 1)\n",
    "        \n",
    "        # Temporal prediction: map the time dimension from T_in to T_out per pixel\n",
    "        x = x.squeeze(-1)         # (batch, T_in, H, W)\n",
    "        x = x.permute(0, 2, 3, 1)   # (batch, H, W, T_in)\n",
    "        x = self.temporal_fc(x)     # (batch, H, W, T_out)\n",
    "        x = x.permute(0, 3, 1, 2).unsqueeze(-1)  # (batch, T_out, H, W, 1)\n",
    "        return x\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Physics-Informed Loss Functions using autograd\n",
    "# =============================================================================\n",
    "def compute_velocity_torch(w):\n",
    "    \"\"\"\n",
    "    Computes velocity (u, v) from vorticity w using FFT.\n",
    "    \n",
    "    Input:\n",
    "        w: Tensor of shape (batch, H, W) representing vorticity.\n",
    "    Output:\n",
    "        u, v: Tensors of shape (batch, H, W) representing velocity components.\n",
    "    \"\"\"\n",
    "    batch, H, W = w.shape\n",
    "    device = w.device\n",
    "    # Create frequency grids for H and W\n",
    "    kx = torch.fft.fftfreq(H, d=1.0/H).to(device) * 2 * np.pi\n",
    "    ky = torch.fft.fftfreq(W, d=1.0/W).to(device) * 2 * np.pi\n",
    "    kx, ky = torch.meshgrid(kx, ky, indexing='ij')\n",
    "    # Expand to batch dimension\n",
    "    kx = kx.unsqueeze(0)  # (1, H, W)\n",
    "    ky = ky.unsqueeze(0)  # (1, H, W)\n",
    "    # Compute squared wave number\n",
    "    ksq = kx**2 + ky**2\n",
    "    ksq[0,0,0] = 1.0  # avoid division by zero\n",
    "    \n",
    "    # Compute Fourier transform of vorticity\n",
    "    w_hat = torch.fft.fft2(w)  # (batch, H, W)\n",
    "    # Compute stream function in Fourier domain\n",
    "    psi_hat = -w_hat / ksq\n",
    "    psi_hat[:,0,0] = 0.0  # set the zero-frequency component to zero\n",
    "    # Compute velocity components in Fourier domain\n",
    "    u_hat = 1j * ky * psi_hat  # u = ∂_y psi\n",
    "    v_hat = -1j * kx * psi_hat  # v = -∂_x psi\n",
    "    # Inverse FFT to get velocities in spatial domain\n",
    "    u = torch.real(torch.fft.ifft2(u_hat))\n",
    "    v = torch.real(torch.fft.ifft2(v_hat))\n",
    "    return u, v\n",
    "\n",
    "def compute_PDE_loss_autograd(pred, nu):\n",
    "    \"\"\"\n",
    "    Computes the PDE loss for the predicted vorticity using autograd to obtain\n",
    "    time and spatial derivatives.\n",
    "    \n",
    "    pred: (batch, T_out, H, W, 1) predicted vorticity.\n",
    "    nu: viscosity coefficient.\n",
    "    \n",
    "    PDE: ∂ₜw + u · ∇w = ν Δw + f(x), where \n",
    "         f(x) = 0.1*(sin(2π(x+y)) + cos(2π(x+y))).\n",
    "    \n",
    "    This function creates coordinate tensors on a uniform grid over [0,1] (for time, x, and y)\n",
    "    with requires_grad=True, then \"injects\" these coordinates into the output by adding a zero term.\n",
    "    This ensures that w becomes a function of these coordinates, allowing autograd to compute the \n",
    "    derivatives with respect to time and space. The PDE residual is then computed and its mean \n",
    "    squared error is returned.\n",
    "    \"\"\"\n",
    "    batch, T_out, H, W, _ = pred.shape\n",
    "    # Remove the last dimension: w: (batch, T_out, H, W)\n",
    "    w = pred.squeeze(-1)\n",
    "    device = pred.device\n",
    "\n",
    "    # Create coordinate tensors (uniform grid on [0,1]) with requires_grad=True\n",
    "    t_coords = torch.linspace(0, 1, steps=T_out, device=device, requires_grad=True).view(1, T_out, 1, 1).expand(batch, T_out, H, W)\n",
    "    x_coords = torch.linspace(0, 1, steps=W, device=device, requires_grad=True).view(1, 1, 1, W).expand(batch, T_out, H, W)\n",
    "    y_coords = torch.linspace(0, 1, steps=H, device=device, requires_grad=True).view(1, 1, H, 1).expand(batch, T_out, H, W)\n",
    "\n",
    "    # Inject coordinate dependency into w by adding a zero term\n",
    "    w = w + 0 * (t_coords + x_coords + y_coords)\n",
    "    \n",
    "    grad_outputs = torch.ones_like(w)\n",
    "    dw_dt = torch.autograd.grad(w, t_coords, grad_outputs=grad_outputs, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "    if dw_dt is None:\n",
    "        dw_dt = torch.zeros_like(w)\n",
    "    dw_dx = torch.autograd.grad(w, x_coords, grad_outputs=grad_outputs, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "    if dw_dx is None:\n",
    "        dw_dx = torch.zeros_like(w)\n",
    "    dw_dy = torch.autograd.grad(w, y_coords, grad_outputs=grad_outputs, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "    if dw_dy is None:\n",
    "        dw_dy = torch.zeros_like(w)\n",
    "    \n",
    "    d2w_dx2 = torch.autograd.grad(dw_dx, x_coords, grad_outputs=grad_outputs, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "    if d2w_dx2 is None:\n",
    "        d2w_dx2 = torch.zeros_like(w)\n",
    "    d2w_dy2 = torch.autograd.grad(dw_dy, y_coords, grad_outputs=grad_outputs, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "    if d2w_dy2 is None:\n",
    "        d2w_dy2 = torch.zeros_like(w)\n",
    "    lap = d2w_dx2 + d2w_dy2  # Laplacian Δw\n",
    "\n",
    "    # Compute velocity using the FFT-based method (autograd-compatible)\n",
    "    u, v = compute_velocity_torch(w)  # each: (batch, H, W)\n",
    "    # Expand u and v to (batch, T_out, H, W)\n",
    "    u = u.unsqueeze(1).expand(-1, T_out, -1, -1)\n",
    "    v = v.unsqueeze(1).expand(-1, T_out, -1, -1)\n",
    "    \n",
    "    adv = u * dw_dx + v * dw_dy\n",
    "    f_val = 0.1 * (torch.sin(2*np.pi*(x_coords + y_coords)) + torch.cos(2*np.pi*(x_coords + y_coords)))\n",
    "    r = dw_dt + adv - nu * lap - f_val\n",
    "    loss_pde = torch.mean(r**2)\n",
    "    return loss_pde\n",
    "\n",
    "\n",
    "\n",
    "def compute_BC_loss(pred):\n",
    "    \"\"\"\n",
    "    Computes the boundary condition loss for predicted vorticity,\n",
    "    enforcing periodic boundaries.\n",
    "    pred: (batch, T_out, H, W, 1)\n",
    "    \"\"\"\n",
    "    batch, T_out, H, W, _ = pred.shape\n",
    "    pred = pred.squeeze(-1)\n",
    "    loss_bc = 0.0\n",
    "    bc_left = pred[:, :, :, 0]\n",
    "    bc_right = pred[:, :, :, -1]\n",
    "    loss_bc += torch.mean((bc_left - bc_right)**2)\n",
    "    bc_top = pred[:, :, 0, :]\n",
    "    bc_bottom = pred[:, :, -1, :]\n",
    "    loss_bc += torch.mean((bc_top - bc_bottom)**2)\n",
    "    return loss_bc\n",
    "\n",
    "def compute_IC_loss(pred, ic):\n",
    "    \"\"\"\n",
    "    Computes the initial condition loss.\n",
    "    pred: (batch, T_out, H, W, 1) - we use the first predicted frame.\n",
    "    ic: (batch, T_out, H, W, 1) - we assume the first frame is the initial condition.\n",
    "    \"\"\"\n",
    "    return F.mse_loss(pred[:,0,:,:,:], ic[:,0,:,:,:])\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Training Loop with Physics-Informed Loss (using autograd for PDE loss)\n",
    "# =============================================================================\n",
    "def train_model_3d(model, train_loader, test_loader, epochs=300, lambda_phys=1.0, lambda_bc=1.0, lambda_ic=1.0, dt_out=1.0):\n",
    "    \"\"\"\n",
    "    Trains the model with a combined loss:\n",
    "      Total Loss = Data Loss + lambda_phys * PDE Loss + lambda_bc * BC Loss + lambda_ic * IC Loss.\n",
    "    \n",
    "    lambda_phys: weight for the physics (PDE) loss.\n",
    "    lambda_bc: weight for the boundary condition loss.\n",
    "    lambda_ic: weight for the initial condition loss.\n",
    "    dt_out: time interval for the output (assumed to be 1.0 for integer target times).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Spatial grid parameters (assuming domain [0,1] for a 64x64 grid)\n",
    "    H_grid = 64\n",
    "    W_grid = 64\n",
    "    dx = 1.0 / W_grid\n",
    "    dy = 1.0 / H_grid\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for input_data, target in train_loader:\n",
    "            # input_data: (batch, T_in, H, W, 4), target: (batch, T_out, H, W, 1)\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_data)  # (batch, T_out, H, W, 1)\n",
    "            loss_data = loss_fn(output, target)\n",
    "            loss_phys = compute_PDE_loss_autograd(output, nu)\n",
    "            loss_bc = compute_BC_loss(output)\n",
    "            loss_ic = compute_IC_loss(output, target)\n",
    "            loss = loss_data + lambda_phys * loss_phys + lambda_bc * loss_bc + lambda_ic * loss_ic\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * input_data.size(0)\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(total_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss_test = 0.0\n",
    "        with torch.no_grad():\n",
    "            for input_data, target in test_loader:\n",
    "                input_data = input_data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(input_data)\n",
    "                loss_data = loss_fn(output, target)\n",
    "                loss_phys = compute_PDE_loss_autograd(output, nu)\n",
    "                loss_bc = compute_BC_loss(output)\n",
    "                loss_ic = compute_IC_loss(output, target)\n",
    "                loss = loss_data + lambda_phys * loss_phys + lambda_bc * loss_bc + lambda_ic * loss_ic\n",
    "                total_loss_test += loss.item() * input_data.size(0)\n",
    "        total_loss_test /= len(test_loader.dataset)\n",
    "        test_losses.append(total_loss_test)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch:03d}: Train Loss = {total_loss:.6f}, Test Loss = {total_loss_test:.6f}\")\n",
    "    \n",
    "        if epoch % 100 == 0:\n",
    "            plt.figure(figsize=(8,6))\n",
    "            plt.plot(train_losses, label=\"Train Loss\")\n",
    "            plt.plot(test_losses, label=\"Test Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Total Loss\")\n",
    "            plt.yscale('log')\n",
    "            plt.legend()\n",
    "            plt.title(\"Training and Test Loss Curves (FNO3d with Physics Loss) [Log Scale]\")\n",
    "            plt.show()\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Fine-tuning using L-BFGS optimizer (with physics-informed loss)\n",
    "# =============================================================================\n",
    "def fine_tune_lbfgs(model, train_loader, test_loader, epochs=50, lambda_phys=1.0, lambda_bc=1.0, lambda_ic=1.0, dt_out=1.0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), lr=0.1, max_iter=20)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    H_grid = 64\n",
    "    W_grid = 64\n",
    "    dx = 1.0 / W_grid\n",
    "    dy = 1.0 / H_grid\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        total_loss_train = 0.0\n",
    "        for input_data, target in train_loader:\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                output = model(input_data)\n",
    "                loss_data = loss_fn(output, target)\n",
    "                loss_phys = compute_PDE_loss_autograd(output, nu)\n",
    "                loss_bc = compute_BC_loss(output)\n",
    "                loss_ic = compute_IC_loss(output, target)\n",
    "                loss = loss_data + lambda_phys * loss_phys + lambda_bc * loss_bc + lambda_ic * loss_ic\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            loss = optimizer.step(closure)\n",
    "            total_loss_train += loss.item() * input_data.size(0)\n",
    "        total_loss_train /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss_test = 0.0\n",
    "        with torch.no_grad():\n",
    "            for input_data, target in test_loader:\n",
    "                input_data = input_data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(input_data)\n",
    "                loss_data = loss_fn(output, target)\n",
    "                loss_phys = compute_PDE_loss_autograd(output, nu)\n",
    "                loss_bc = compute_BC_loss(output)\n",
    "                loss_ic = compute_IC_loss(output, target)\n",
    "                loss = loss_data + lambda_phys * loss_phys + lambda_bc * loss_bc + lambda_ic * loss_ic\n",
    "                total_loss_test += loss.item() * input_data.size(0)\n",
    "        total_loss_test /= len(test_loader.dataset)\n",
    "        model.train()\n",
    "        print(f\"L-BFGS Epoch {epoch:03d}: Train Loss = {total_loss_train:.6f}, Test Loss = {total_loss_test:.6f}\")\n",
    "    return total_loss_train, total_loss_test\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Discrete Visualization\n",
    "# =============================================================================\n",
    "def visualize_sample_3d(model, test_loader):\n",
    "    \"\"\"\n",
    "    Visualizes discrete predicted and ground truth frames.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_data, target = next(iter(test_loader))\n",
    "        input_data = input_data.to(device)\n",
    "        pred = model(input_data)\n",
    "    sample_target = target[0].cpu().numpy()  # (T_out, H, W, 1)\n",
    "    sample_pred = pred[0].cpu().numpy()      # (T_out, H, W, 1)\n",
    "    times = [20, 30, 40, 49]  # target time labels\n",
    "    T_out = sample_target.shape[0]\n",
    "    plt.figure(figsize=(16, 4*T_out))\n",
    "    for i in range(T_out):\n",
    "        plt.subplot(T_out, 2, 2*i+1)\n",
    "        plt.title(f\"Ground Truth at t = {times[i]}\")\n",
    "        plt.imshow(sample_target[i, :, :, 0], cmap=\"RdBu\", origin=\"lower\")\n",
    "        plt.colorbar()\n",
    "        plt.subplot(T_out, 2, 2*i+2)\n",
    "        plt.title(f\"Prediction at t = {times[i]}\")\n",
    "        plt.imshow(sample_pred[i, :, :, 0], cmap=\"RdBu\", origin=\"lower\")\n",
    "        plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 9. Full Pipeline Execution\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # PDE simulation parameters\n",
    "    N = 64         # Spatial resolution: 64x64\n",
    "    dt = 0.005     # Time step (small for stability)\n",
    "    t_final = 50   # Final time\n",
    "    nu = 1e-3      # Viscosity\n",
    "    \n",
    "    # Dataset time ranges\n",
    "    T_in_range = (0, 10)          # Input: t=0 ~ 10 (integer snapshots)\n",
    "    T_out_times = [20, 30, 40, 49] # Target times (example)\n",
    "    \n",
    "    num_train = 5   # Number of training samples\n",
    "    num_test = 1      # Number of test samples\n",
    "    \n",
    "    dataset_solver_train = NavierStokesDatasetFromSolver(num_samples=num_train, N=N, dt=dt, t_final=t_final, nu=nu, \n",
    "                                                          T_in_range=T_in_range, T_out_times=T_out_times)\n",
    "    dataset_solver_test  = NavierStokesDatasetFromSolver(num_samples=num_test, N=N, dt=dt, t_final=t_final, nu=nu, \n",
    "                                                          T_in_range=T_in_range, T_out_times=T_out_times)\n",
    "    \n",
    "    train_loader = DataLoader(dataset_solver_train, batch_size=2, shuffle=True)\n",
    "    test_loader = DataLoader(dataset_solver_test, batch_size=2, shuffle=False)\n",
    "    \n",
    "    modes_t, modes_x, modes_y = 4, 16, 16\n",
    "    width = 32\n",
    "    T_out = len(T_out_times)\n",
    "    T_in = len(np.arange(T_in_range[0], T_in_range[1]+1))  # e.g., 11 for t=0~10\n",
    "    model = FNO3d(modes_t, modes_x, modes_y, width, T_in, T_out)\n",
    "    \n",
    "    # Pre-training with Adam optimizer using autograd-based PDE loss\n",
    "    train_losses, test_losses = train_model_3d(model, train_loader, test_loader, epochs=500, lambda_phys=1.0, dt_out=1.0)\n",
    "    \n",
    "    # Fine-tuning with L-BFGS optimizer using autograd-based PDE loss, BC loss and IC loss\n",
    "    print(\"Starting L-BFGS fine-tuning...\")\n",
    "    fine_tune_lbfgs(model, train_loader, test_loader, epochs=50, lambda_phys=1.0, dt_out=1.0)\n",
    "    \n",
    "    # Discrete visualization of predicted vs. ground truth frames\n",
    "    visualize_sample_3d(model, test_loader)\n",
    "    \n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
